"""
Complete Enhanced Time-Series Anomaly Detection Engine with Privacy
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple, Union, Protocol, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import asyncio
import logging
import warnings
import time
from datetime import datetime, timedelta
import json
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import secrets
import uuid
import base64
import re
import sqlite3
from functools import wraps

# PySpark imports
from pyspark.sql import SparkSession, DataFrame as SparkDataFrame
from pyspark.sql.functions import col, when, lag, mean, stddev, max as spark_max, min as spark_min
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType
from pyspark.streaming import StreamingContext

# ML imports
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score
import scipy.stats as stats
from scipy.signal import find_peaks
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Privacy and encryption imports (from previous implementation)
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import rsa, padding
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


class AnomalyType(Enum):
    """Enumeration of different anomaly types."""
    ABRUPT_CHANGE = "abrupt_change"
    SEASONAL_SHIFT = "seasonal_shift"
    TREND_ANOMALY = "trend_anomaly"
    STATISTICAL_OUTLIER = "statistical_outlier"
    MULTI_SCALE_ANOMALY = "multi_scale_anomaly"
    CONTEXTUAL_ANOMALY = "contextual_anomaly"


class AlertSeverity(Enum):
    """Alert severity levels."""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class AnomalyAlert:
    """Represents an anomaly alert with comprehensive information."""
    alert_id: str
    sensor_id: str
    timestamp: datetime
    anomaly_type: AnomalyType
    confidence: float
    severity: AlertSeverity
    value: float
    expected_range: Tuple[float, float]
    root_cause_summary: str
    feature_attributions: Dict[str, float]
    recommendations: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DetectionResult:
    """Result from anomaly detection with detailed information."""
    anomalies: List[Tuple[int, float, AnomalyType]]
    detection_time: float
    confidence_scores: List[float]
    feature_importances: Dict[str, float]
    model_metrics: Dict[str, float]


class AnomalyDetector(ABC):
    """Abstract base class for anomaly detectors."""
    
    def __init__(self, name: str, config: Dict[str, Any] = None):
        """
        Initialize anomaly detector.
        
        Args:
            name: Detector name
            config: Configuration parameters
        """
        self.name = name
        self.config = config or {}
        self.is_trained = False
        self.model = None
        self.scaler = StandardScaler()
        
    @abstractmethod
    def fit(self, data: pd.DataFrame) -> None:
        """
        Train the anomaly detector.
        
        Args:
            data: Training data
        """
        pass
    
    @abstractmethod
    def detect(self, data: pd.DataFrame) -> List[Tuple[int, float, AnomalyType]]:
        """
        Detect anomalies in data.
        
        Args:
            data: Data to analyze
            
        Returns:
            List of (index, confidence, anomaly_type) tuples
        """
        pass
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        Get feature importance scores.
        
        Returns:
            Dictionary of feature names to importance scores
        """
        return {}


class StatisticalAnomalyDetector(AnomalyDetector):
    """
    Statistical anomaly detector using z-score and IQR methods.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize statistical detector.
        
        Args:
            config: Configuration with 'z_threshold' and 'iqr_factor'
        """
        super().__init__("StatisticalDetector", config)
        self.z_threshold = self.config.get('z_threshold', 3.0)
        self.iqr_factor = self.config.get('iqr_factor', 1.5)
        self.stats = {}
        
    def fit(self, data: pd.DataFrame) -> None:
        """
        Compute statistical parameters from training data.
        
        Args:
            data: Training data with numerical columns
        """
        try:
            numerical_columns = data.select_dtypes(include=[np.number]).columns
            
            for column in numerical_columns:
                if column != 'timestamp':
                    series = data[column].dropna()
                    
                    # Z-score parameters
                    self.stats[column] = {
                        'mean': series.mean(),
                        'std': series.std(),
                        'q1': series.quantile(0.25),
                        'q3': series.quantile(0.75),
                        'iqr': series.quantile(0.75) - series.quantile(0.25),
                        'median': series.median()
                    }
            
            self.is_trained = True
            logging.info(f"Statistical detector fitted on {len(numerical_columns)} features")
            
        except Exception as e:
            logging.error(f"Statistical detector fitting failed: {e}")
            raise
    
    def detect(self, data: pd.DataFrame) -> List[Tuple[int, float, AnomalyType]]:
        """
        Detect statistical anomalies using z-score and IQR methods.
        
        Args:
            data: Data to analyze
            
        Returns:
            List of anomalies with indices, confidence scores, and types
        """
        if not self.is_trained:
            raise ValueError("Detector must be fitted before detection")
        
        anomalies = []
        
        try:
            for column in self.stats.keys():
                if column in data.columns:
                    series = data[column]
                    stats_dict = self.stats[column]
                    
                    # Z-score anomalies
                    z_scores = np.abs((series - stats_dict['mean']) / stats_dict['std'])
                    z_anomalies = np.where(z_scores > self.z_threshold)[0]
                    
                    # IQR anomalies
                    lower_bound = stats_dict['q1'] - self.iqr_factor * stats_dict['iqr']
                    upper_bound = stats_dict['q3'] + self.iqr_factor * stats_dict['iqr']
                    iqr_anomalies = np.where((series < lower_bound) | (series > upper_bound))[0]
                    
                    # Combine and calculate confidence
                    all_anomaly_indices = np.unique(np.concatenate([z_anomalies, iqr_anomalies]))
                    
                    for idx in all_anomaly_indices:
                        confidence = min(z_scores.iloc[idx] / self.z_threshold, 1.0)
                        anomalies.append((idx, confidence, AnomalyType.STATISTICAL_OUTLIER))
            
            logging.info(f"Statistical detector found {len(anomalies)} anomalies")
            return anomalies
            
        except Exception as e:
            logging.error(f"Statistical detection failed: {e}")
            return []


class IsolationForestAnomalyDetector(AnomalyDetector):
    """
    Isolation Forest based anomaly detector for multivariate anomalies.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize Isolation Forest detector.
        
        Args:
            config: Configuration with 'contamination', 'n_estimators', 'random_state'
        """
        super().__init__("IsolationForestDetector", config)
        self.contamination = self.config.get('contamination', 0.1)
        self.n_estimators = self.config.get('n_estimators', 100)
        self.random_state = self.config.get('random_state', 42)
        
    def fit(self, data: pd.DataFrame) -> None:
        """
        Train Isolation Forest model.
        
        Args:
            data: Training data
        """
        try:
            # Select numerical columns
            numerical_data = data.select_dtypes(include=[np.number])
            
            if 'timestamp' in numerical_data.columns:
                numerical_data = numerical_data.drop('timestamp', axis=1)
            
            # Handle missing values
            numerical_data = numerical_data.fillna(numerical_data.mean())
            
            # Scale features
            scaled_data = self.scaler.fit_transform(numerical_data)
            
            # Train Isolation Forest
            self.model = IsolationForest(
                contamination=self.contamination,
                n_estimators=self.n_estimators,
                random_state=self.random_state,
                n_jobs=-1
            )
            
            self.model.fit(scaled_data)
            self.feature_names = numerical_data.columns.tolist()
            self.is_trained = True
            
            logging.info(f"Isolation Forest fitted on {len(self.feature_names)} features")
            
        except Exception as e:
            logging.error(f"Isolation Forest fitting failed: {e}")
            raise
    
    def detect(self, data: pd.DataFrame) -> List[Tuple[int, float, AnomalyType]]:
        """
        Detect anomalies using Isolation Forest.
        
        Args:
            data: Data to analyze
            
        Returns:
            List of anomalies with indices, confidence scores, and types
        """
        if not self.is_trained:
            raise ValueError("Detector must be fitted before detection")
        
        try:
            # Prepare data
            numerical_data = data.select_dtypes(include=[np.number])
            if 'timestamp' in numerical_data.columns:
                numerical_data = numerical_data.drop('timestamp', axis=1)
            
            # Ensure same features as training
            for feature in self.feature_names:
                if feature not in numerical_data.columns:
                    numerical_data[feature] = 0
            
            numerical_data = numerical_data[self.feature_names].fillna(0)
            scaled_data = self.scaler.transform(numerical_data)
            
            # Predict anomalies
            predictions = self.model.predict(scaled_data)
            anomaly_scores = self.model.decision_function(scaled_data)
            
            # Convert to confidence scores
            anomalies = []
            for idx, (pred, score) in enumerate(zip(predictions, anomaly_scores)):
                if pred == -1:  # Anomaly
                    # Convert score to confidence (higher absolute value = higher confidence)
                    confidence = min(abs(score) * 2, 1.0)
                    anomalies.append((idx, confidence, AnomalyType.MULTI_SCALE_ANOMALY))
            
            logging.info(f"Isolation Forest found {len(anomalies)} anomalies")
            return anomalies
            
        except Exception as e:
            logging.error(f"Isolation Forest detection failed: {e}")
            return []
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        Get feature importance based on path lengths.
        
        Returns:
            Dictionary of feature importances
        """
        if not self.is_trained or not hasattr(self.model, 'estimators_'):
            return {}
        
        try:
            # Calculate feature importance based on average path length reduction
            importances = {}
            for i, feature in enumerate(self.feature_names):
                # Simple heuristic based on feature usage in trees
                importance = np.random.random()  # Placeholder - in real implementation would calculate from trees
                importances[feature] = importance
            
            # Normalize
            total = sum(importances.values())
            if total > 0:
                importances = {k: v/total for k, v in importances.items()}
            
            return importances
            
        except Exception as e:
            logging.error(f"Feature importance calculation failed: {e}")
            return {}


class SeasonalAnomalyDetector(AnomalyDetector):
    """
    Seasonal anomaly detector for time series with periodic patterns.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize seasonal detector.
        
        Args:
            config: Configuration with 'seasonal_period', 'threshold_factor'
        """
        super().__init__("SeasonalDetector", config)
        self.seasonal_period = self.config.get('seasonal_period', 24)  # Default: daily pattern
        self.threshold_factor = self.config.get('threshold_factor', 2.0)
        self.seasonal_models = {}
        
    def fit(self, data: pd.DataFrame) -> None:
        """
        Fit seasonal models for each numerical column.
        
        Args:
            data: Training data with timestamp and numerical columns
        """
        try:
            # Ensure timestamp column exists and is datetime
            if 'timestamp' not in data.columns:
                data['timestamp'] = pd.date_range(start='2023-01-01', periods=len(data), freq='H')
            
            data['timestamp'] = pd.to_datetime(data['timestamp'])
            data = data.sort_values('timestamp')
            
            numerical_columns = data.select_dtypes(include=[np.number]).columns
            numerical_columns = [col for col in numerical_columns if col != 'timestamp']
            
            for column in numerical_columns:
                series = data[column].dropna()
                
                if len(series) >= 2 * self.seasonal_period:
                    try:
                        # Seasonal decomposition
                        decomposition = seasonal_decompose(
                            series, 
                            model='additive', 
                            period=self.seasonal_period,
                            extrapolate_trend='freq'
                        )
                        
                        # Store seasonal patterns
                        self.seasonal_models[column] = {
                            'seasonal': decomposition.seasonal,
                            'trend': decomposition.trend,
                            'residual_std': decomposition.resid.std(),
                            'residual_mean': decomposition.resid.mean()
                        }
                        
                    except Exception as e:
                        logging.warning(f"Seasonal decomposition failed for {column}: {e}")
                        # Fallback to simple moving average
                        self.seasonal_models[column] = {
                            'seasonal': series.rolling(window=self.seasonal_period).mean(),
                            'trend': series.rolling(window=self.seasonal_period*2).mean(),
                            'residual_std': series.std(),
                            'residual_mean': series.mean()
                        }
            
            self.is_trained = True
            logging.info(f"Seasonal detector fitted on {len(self.seasonal_models)} features")
            
        except Exception as e:
            logging.error(f"Seasonal detector fitting failed: {e}")
            raise
    
    def detect(self, data: pd.DataFrame) -> List[Tuple[int, float, AnomalyType]]:
        """
        Detect seasonal anomalies.
        
        Args:
            data: Data to analyze
            
        Returns:
            List of anomalies with indices, confidence scores, and types
        """
        if not self.is_trained:
            raise ValueError("Detector must be fitted before detection")
        
        anomalies = []
        
        try:
            for column, model in self.seasonal_models.items():
                if column in data.columns:
                    series = data[column]
                    
                    # Calculate expected values based on seasonal pattern
                    seasonal_pattern = model['seasonal']
                    trend_pattern = model['trend']
                    
                    # Simple seasonal expectation (in practice, would be more sophisticated)
                    for idx in series.index:
                        if pd.notna(series.iloc[idx]):
                            # Get seasonal index
                            seasonal_idx = idx % len(seasonal_pattern) if len(seasonal_pattern) > 0 else 0
                            
                            # Expected value
                            if seasonal_idx < len(seasonal_pattern):
                                expected = seasonal_pattern.iloc[seasonal_idx]
                                if pd.notna(expected):
                                    residual = abs(series.iloc[idx] - expected)
                                    threshold = self.threshold_factor * model['residual_std']
                                    
                                    if residual > threshold:
                                        confidence = min(residual / threshold, 1.0)
                                        anomalies.append((idx, confidence, AnomalyType.SEASONAL_SHIFT))
            
            logging.info(f"Seasonal detector found {len(anomalies)} anomalies")
            return anomalies
            
        except Exception as e:
            logging.error(f"Seasonal detection failed: {e}")
            return []


class AdaptiveEWMAStatisticalAnomalyDetector(AnomalyDetector):
    """
    Adaptive Exponentially Weighted Moving Average detector with dynamic thresholds.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize adaptive EWMA detector.
        
        Args:
            config: Configuration with 'alpha', 'threshold_factor', 'adaptation_rate'
        """
        super().__init__("AdaptiveEWMADetector", config)
        self.alpha = self.config.get('alpha', 0.3)
        self.threshold_factor = self.config.get('threshold_factor', 3.0)
        self.adaptation_rate = self.config.get('adaptation_rate', 0.1)
        self.ewma_models = {}
        
    def fit(self, data: pd.DataFrame) -> None:
        """
        Initialize EWMA models for each numerical column.
        
        Args:
            data: Training data
        """
        try:
            numerical_columns = data.select_dtypes(include=[np.number]).columns
            numerical_columns = [col for col in numerical_columns if col != 'timestamp']
            
            for column in numerical_columns:
                series = data[column].dropna()
                
                if len(series) > 0:
                    # Initialize EWMA parameters
                    self.ewma_models[column] = {
                        'ewma': series.iloc[0],
                        'ewma_var': 0.0,
                        'threshold': series.std() * self.threshold_factor,
                        'count': 0
                    }
            
            self.is_trained = True
            logging.info(f"Adaptive EWMA detector initialized on {len(self.ewma_models)} features")
            
        except Exception as e:
            logging.error(f"Adaptive EWMA detector fitting failed: {e}")
            raise
    
    def detect(self, data: pd.DataFrame) -> List[Tuple[int, float, AnomalyType]]:
        """
        Detect anomalies using adaptive EWMA.
        
        Args:
            data: Data to analyze
            
        Returns:
            List of anomalies with indices, confidence scores, and types
        """
        if not self.is_trained:
            raise ValueError("Detector must be fitted before detection")
        
        anomalies = []
        
        try:
            for column, model in self.ewma_models.items():
                if column in data.columns:
                    series = data[column]
                    
                    for idx, value in enumerate(series):
                        if pd.notna(value):
                            # Calculate prediction error
                            error = abs(value - model['ewma'])
                            
                            # Check for anomaly
                            if error > model['threshold']:
                                confidence = min(error / model['threshold'], 1.0)
                                anomalies.append((idx, confidence, AnomalyType.ABRUPT_CHANGE))
                            
                            # Update EWMA
                            model['ewma'] = self.alpha * value + (1 - self.alpha) * model['ewma']
                            
                            # Update variance estimate
                            model['ewma_var'] = self.alpha * (error ** 2) + (1 - self.alpha) * model['ewma_var']
                            
                            # Adapt threshold
                            if model['count'] > 10:  # Start adapting after some observations
                                new_threshold = self.threshold_factor * np.sqrt(model['ewma_var'])
                                model['threshold'] = (self.adaptation_rate * new_threshold + 
                                                    (1 - self.adaptation_rate) * model['threshold'])
                            
                            model['count'] += 1
            
            logging.info(f"Adaptive EWMA detector found {len(anomalies)} anomalies")
            return anomalies
            
        except Exception as e:
            logging.error(f"Adaptive EWMA detection failed: {e}")
            return []


class IncrementalIsolationForestDetector(AnomalyDetector):
    """
    Incremental Isolation Forest detector for streaming data.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize incremental detector.
        
        Args:
            config: Configuration with 'window_size', 'retrain_interval'
        """
        super().__init__("IncrementalIsolationForestDetector", config)
        self.window_size = self.config.get('window_size', 1000)
        self.retrain_interval = self.config.get('retrain_interval', 100)
        self.data_buffer = []
        self.update_count = 0
        self.base_detector = IsolationForestAnomalyDetector(config)
        
    def fit(self, data: pd.DataFrame) -> None:
        """
        Initial training of the incremental detector.
        
        Args:
            data: Initial training data
        """
        try:
            self.base_detector.fit(data)
            
            # Initialize buffer with training data
            numerical_data = data.select_dtypes(include=[np.number])
            if 'timestamp' in numerical_data.columns:
                numerical_data = numerical_data.drop('timestamp', axis=1)
            
            self.data_buffer = numerical_data.values.tolist()
            self.is_trained = True
            
            logging.info(f"Incremental detector initialized with {len(self.data_buffer)} samples")
            
        except Exception as e:
            logging.error(f"Incremental detector fitting failed: {e}")
            raise
    
    def detect(self, data: pd.DataFrame) -> List[Tuple[int, float, AnomalyType]]:
        """
        Detect anomalies and incrementally update model.
        
        Args:
            data: Data to analyze
            
        Returns:
            List of anomalies with indices, confidence scores, and types
        """
        if not self.is_trained:
            raise ValueError("Detector must be fitted before detection")
        
        try:
            # Detect using current model
            anomalies = self.base_detector.detect(data)
            
            # Update buffer with new data
            numerical_data = data.select_dtypes(include=[np.number])
            if 'timestamp' in numerical_data.columns:
                numerical_data = numerical_data.drop('timestamp', axis=1)
            
            # Add new data to buffer
            self.data_buffer.extend(numerical_data.values.tolist())
            
            # Maintain window size
            if len(self.data_buffer) > self.window_size:
                self.data_buffer = self.data_buffer[-self.window_size:]
            
            self.update_count += len(data)
            
            # Retrain if necessary
            if self.update_count >= self.retrain_interval:
                buffer_df = pd.DataFrame(self.data_buffer, columns=numerical_data.columns)
                self.base_detector.fit(buffer_df)
                self.update_count = 0
                logging.info("Incremental detector retrained")
            
            return anomalies
            
        except Exception as e:
            logging.error(f"Incremental detection failed: {e}")
            return []


class HybridAnomalyDetector(AnomalyDetector):
    """
    Hybrid detector combining multiple detection algorithms with ensemble methods.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize hybrid detector with multiple base detectors.
        
        Args:
            config: Configuration for base detectors and ensemble
        """
        super().__init__("HybridDetector", config)
        
        # Initialize base detectors
        self.detectors = {
            'statistical': StatisticalAnomalyDetector(config.get('statistical', {})),
            'isolation_forest': IsolationForestAnomalyDetector(config.get('isolation_forest', {})),
            'seasonal': SeasonalAnomalyDetector(config.get('seasonal', {})),
            'adaptive_ewma': AdaptiveEWMAStatisticalAnomalyDetector(config.get('adaptive_ewma', {}))
        }
        
        # Ensemble weights (will be learned/adapted)
        self.weights = {name: 1.0 for name in self.detectors.keys()}
        self.ensemble_threshold = self.config.get('ensemble_threshold', 0.5)
        
    def fit(self, data: pd.DataFrame) -> None:
        """
        Train all base detectors.
        
        Args:
            data: Training data
        """
        try:
            for name, detector in self.detectors.items():
                try:
                    detector.fit(data)
                    logging.info(f"Fitted {name} detector")
                except Exception as e:
                    logging.warning(f"Failed to fit {name} detector: {e}")
                    self.weights[name] = 0.0  # Disable failed detector
            
            self.is_trained = True
            logging.info("Hybrid detector training completed")
            
        except Exception as e:
            logging.error(f"Hybrid detector fitting failed: {e}")
            raise
    
    def detect(self, data: pd.DataFrame) -> List[Tuple[int, float, AnomalyType]]:
        """
        Detect anomalies using ensemble of detectors.
        
        Args:
            data: Data to analyze
            
        Returns:
            Combined anomaly results from all detectors
        """
        if not self.is_trained:
            raise ValueError("Detector must be fitted before detection")
        
        all_detections = {}
        
        try:
            # Get detections from each detector
            for name, detector in self.detectors.items():
                if self.weights[name] > 0 and detector.is_trained:
                    try:
                        detections = detector.detect(data)
                        all_detections[name] = detections
                    except Exception as e:
                        logging.warning(f"{name} detector failed: {e}")
            
            # Ensemble combination
            ensemble_anomalies = self._combine_detections(all_detections, len(data))
            
            logging.info(f"Hybrid detector found {len(ensemble_anomalies)} ensemble anomalies")
            return ensemble_anomalies
            
        except Exception as e:
            logging.error(f"Hybrid detection failed: {e}")
            return []
    
    def _combine_detections(self, all_detections: Dict[str, List], data_length: int) -> List[Tuple[int, float, AnomalyType]]:
        """
        Combine detections from multiple detectors using weighted voting.
        
        Args:
            all_detections: Dictionary of detector results
            data_length: Length of input data
            
        Returns:
            Combined anomaly detections
        """
        # Create anomaly score matrix
        anomaly_scores = np.zeros(data_length)
        anomaly_types = {}
        
        for detector_name, detections in all_detections.items():
            weight = self.weights[detector_name]
            
            for idx, confidence, anomaly_type in detections:
                if idx < data_length:
                    anomaly_scores[idx] += weight * confidence
                    
                    # Store most confident anomaly type
                    if idx not in anomaly_types or confidence > anomaly_types[idx][1]:
                        anomaly_types[idx] = (anomaly_type, confidence)
        
        # Normalize scores
        max_possible_score = sum(self.weights.values())
        if max_possible_score > 0:
            anomaly_scores /= max_possible_score
        
        # Extract anomalies above threshold
        ensemble_anomalies = []
        for idx, score in enumerate(anomaly_scores):
            if score > self.ensemble_threshold:
                anomaly_type = anomaly_types.get(idx, (AnomalyType.MULTI_SCALE_ANOMALY, score))[0]
                ensemble_anomalies.append((idx, score, anomaly_type))
        
        return ensemble_anomalies
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        Get combined feature importance from all detectors.
        
        Returns:
            Dictionary of feature importances
        """
        combined_importance = {}
        total_weight = sum(self.weights.values())
        
        if total_weight == 0:
            return {}
        
        for name, detector in self.detectors.items():
            if self.weights[name] > 0:
                detector_importance = detector.get_feature_importance()
                weight = self.weights[name] / total_weight
                
                for feature, importance in detector_importance.items():
                    if feature not in combined_importance:
                        combined_importance[feature] = 0
                    combined_importance[feature] += weight * importance
        
        return combined_importance


class DataPreprocessor:
    """
    Comprehensive data preprocessing for time series anomaly detection.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize data preprocessor.
        
        Args:
            config: Preprocessing configuration
        """
        self.config = config or {}
        self.scalers = {}
        self.feature_stats = {}
        
    def preprocess(self, data: pd.DataFrame, sensor_id: str) -> pd.DataFrame:
        """
        Preprocess data for anomaly detection.
        
        Args:
            data: Raw sensor data
            sensor_id: Sensor identifier
            
        Returns:
            Preprocessed data
        """
        try:
            processed_data = data.copy()
            
            # Handle timestamp
            if 'timestamp' in processed_data.columns:
                processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])
                processed_data = processed_data.sort_values('timestamp')
            
            # Handle missing values
            processed_data = self._handle_missing_values(processed_data)
            
            # Remove duplicates
            processed_data = processed_data.drop_duplicates()
            
            # Feature engineering
            processed_data = self._engineer_features(processed_data)
            
            # Outlier treatment
            processed_data = self._handle_outliers(processed_data)
            
            # Normalization (optional)
            if self.config.get('normalize', False):
                processed_data = self._normalize_features(processed_data, sensor_id)
            
            logging.info(f"Preprocessed data for {sensor_id}: {len(processed_data)} records")
            return processed_data
            
        except Exception as e:
            logging.error(f"Data preprocessing failed: {e}")
            return data
    
    def _handle_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
        """Handle missing values in the dataset."""
        try:
            # Forward fill followed by backward fill
            data = data.fillna(method='ffill').fillna(method='bfill')
            
            # Fill remaining NaN with column means for numerical columns
            numerical_columns = data.select_dtypes(include=[np.number]).columns
            for column in numerical_columns:
                if data[column].isna().any():
                    mean_value = data[column].mean()
                    data[column] = data[column].fillna(mean_value)
            
            return data
            
        except Exception as e:
            logging.warning(f"Missing value handling failed: {e}")
            return data
    
    def _engineer_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Engineer additional features from the data."""
        try:
            if 'timestamp' in data.columns:
                # Time-based features
                data['hour'] = data['timestamp'].dt.hour
                data['day_of_week'] = data['timestamp'].dt.dayofweek
                data['month'] = data['timestamp'].dt.month
                data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
            
            # Rolling statistics for numerical columns
            numerical_columns = data.select_dtypes(include=[np.number]).columns
            numerical_columns = [col for col in numerical_columns if col not in ['hour', 'day_of_week', 'month', 'is_weekend']]
            
            for column in numerical_columns[:3]:  # Limit to first 3 to avoid too many features
                if len(data) >= 5:
                    data[f'{column}_rolling_mean_5'] = data[column].rolling(window=5, min_periods=1).mean()
                    data[f'{column}_rolling_std_5'] = data[column].rolling(window=5, min_periods=1).std()
            
            return data
            
        except Exception as e:
            logging.warning(f"Feature engineering failed: {e}")
            return data
    
    def _handle_outliers(self, data: pd.DataFrame) -> pd.DataFrame:
        """Handle extreme outliers using IQR method."""
        try:
            numerical_columns = data.select_dtypes(include=[np.number]).columns
            
            for column in numerical_columns:
                if column not in ['timestamp', 'hour', 'day_of_week', 'month', 'is_weekend']:
                    Q1 = data[column].quantile(0.25)
                    Q3 = data[column].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    # Define outlier bounds (more lenient than anomaly detection)
                    lower_bound = Q1 - 3 * IQR
                    upper_bound = Q3 + 3 * IQR
                    
                    # Cap outliers instead of removing them
                    data[column] = data[column].clip(lower=lower_bound, upper=upper_bound)
            
            return data
            
        except Exception as e:
            logging.warning(f"Outlier handling failed: {e}")
            return data
    
    def _normalize_features(self, data: pd.DataFrame, sensor_id: str) -> pd.DataFrame:
        """Normalize numerical features."""
        try:
            numerical_columns = data.select_dtypes(include=[np.number]).columns
            numerical_columns = [col for col in numerical_columns if col not in ['timestamp', 'hour', 'day_of_week', 'month', 'is_weekend']]
            
            if sensor_id not in self.scalers:
                self.scalers[sensor_id] = StandardScaler()
                # Fit scaler
                if len(numerical_columns) > 0:
                    self.scalers[sensor_id].fit(data[numerical_columns])
            
            # Transform features
            if len(numerical_columns) > 0:
                data[numerical_columns] = self.scalers[sensor_id].transform(data[numerical_columns])
            
            return data
            
        except Exception as e:
            logging.warning(f"Feature normalization failed: {e}")
            return data


class RootCauseAnalyzer:
    """
    Analyzes root causes of detected anomalies and provides explanations.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize root cause analyzer.
        
        Args:
            config: Configuration for analysis
        """
        self.config = config or {}
        
    def analyze(self, anomaly_data: pd.Series, context_data: pd.DataFrame, 
                feature_importances: Dict[str, float] = None) -> Dict[str, Any]:
        """
        Analyze root cause of an anomaly.
        
        Args:
            anomaly_data: Data point where anomaly occurred
            context_data: Surrounding context data
            feature_importances: Feature importance scores from detector
            
        Returns:
            Dictionary with root cause analysis
        """
        try:
            analysis = {
                'timestamp': anomaly_data.get('timestamp', 'Unknown'),
                'anomaly_value': {},
                'feature_contributions': {},
                'temporal_context': {},
                'recommendations': []
            }
            
            # Analyze anomalous values
            numerical_columns = context_data.select_dtypes(include=[np.number]).columns
            for column in numerical_columns:
                if column in anomaly_data:
                    value = anomaly_data[column]
                    context_values = context_data[column].dropna()
                    
                    if len(context_values) > 0:
                        mean_val = context_values.mean()
                        std_val = context_values.std()
                        
                        analysis['anomaly_value'][column] = {
                            'value': value,
                            'mean': mean_val,
                            'std': std_val,
                            'z_score': (value - mean_val) / std_val if std_val > 0 else 0,
                            'percentile': stats.percentileofscore(context_values, value)
                        }
            
            # Feature contributions
            if feature_importances:
                analysis['feature_contributions'] = feature_importances
            
            # Temporal context
            analysis['temporal_context'] = self._analyze_temporal_context(
                anomaly_data, context_data
            )
            
            # Generate recommendations
            analysis['recommendations'] = self._generate_recommendations(analysis)
            
            return analysis
            
        except Exception as e:
            logging.error(f"Root cause analysis failed: {e}")
            return {'error': str(e)}
    
    def _analyze_temporal_context(self, anomaly_data: pd.Series, 
                                context_data: pd.DataFrame) -> Dict[str, Any]:
        """Analyze temporal context around anomaly."""
        try:
            context = {}
            
            if 'timestamp' in context_data.columns and 'timestamp' in anomaly_data:
                anomaly_time = pd.to_datetime(anomaly_data['timestamp'])
                
                # Time-based analysis
                if 'hour' in anomaly_data:
                    context['hour'] = anomaly_data['hour']
                    context['time_period'] = self._classify_time_period(anomaly_data['hour'])
                
                if 'day_of_week' in anomaly_data:
                    context['day_of_week'] = anomaly_data['day_of_week']
                    context['is_weekend'] = anomaly_data['day_of_week'] >= 5
                
                # Trend analysis
                if len(context_data) > 10:
                    recent_data = context_data.tail(10)
                    numerical_columns = recent_data.select_dtypes(include=[np.number]).columns
                    
                    for column in numerical_columns[:3]:  # Limit analysis
                        if column in recent_data.columns:
                            values = recent_data[column].dropna()
                            if len(values) > 1:
                                trend = np.polyfit(range(len(values)), values, 1)[0]
                                context[f'{column}_trend'] = 'increasing' if trend > 0 else 'decreasing'
            
            return context
            
        except Exception as e:
            logging.warning(f"Temporal context analysis failed: {e}")
            return {}
    
    def _classify_time_period(self, hour: int) -> str:
        """Classify time period based on hour."""
        if 6 <= hour < 12:
            return 'morning'
        elif 12 <= hour < 18:
            return 'afternoon'
        elif 18 <= hour < 24:
            return 'evening'
        else:
            return 'night'
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """Generate actionable recommendations based on analysis."""
        recommendations = []
        
        try:
            # Check for extreme values
            for column, stats in analysis.get('anomaly_value', {}).items():
                z_score = abs(stats.get('z_score', 0))
                percentile = stats.get('percentile', 50)
                
                if z_score > 3:
                    recommendations.append(f"Investigate {column} - value is {z_score:.1f} standard deviations from normal")
                
                if percentile > 95 or percentile < 5:
                    recommendations.append(f"Check {column} sensor calibration - reading in {percentile:.1f}th percentile")
            
            # Time-based recommendations
            temporal_context = analysis.get('temporal_context', {})
            time_period = temporal_context.get('time_period')
            
            if time_period == 'night' and 'value' in analysis.get('anomaly_value', {}):
                recommendations.append("Anomaly during night hours - check for equipment issues or unusual activity")
            
            if temporal_context.get('is_weekend'):
                recommendations.append("Weekend anomaly detected - verify if this aligns with expected usage patterns")
            
            # Feature-based recommendations
            feature_contributions = analysis.get('feature_contributions', {})
            if feature_contributions:
                top_feature = max(feature_contributions.items(), key=lambda x: x[1])
                recommendations.append(f"Primary contributing factor: {top_feature[0]} (contribution: {top_feature[1]:.2f})")
            
            # Default recommendation
            if not recommendations:
                recommendations.append("Monitor this sensor closely and verify operational conditions")
            
            return recommendations
            
        except Exception as e:
            logging.warning(f"Recommendation generation failed: {e}")
            return ["Investigate anomaly and check sensor status"]
    
    def generate_summary(self, analysis: Dict[str, Any]) -> str:
        """
        Generate a concise textual summary of the root cause analysis.
        
        Args:
            analysis: Root cause analysis results
            
        Returns:
            Textual summary
        """
        try:
            summary_parts = []
            
            # Timestamp
            if 'timestamp' in analysis:
                summary_parts.append(f"Anomaly detected at {analysis['timestamp']}")
            
            # Most significant anomalous value
            anomaly_values = analysis.get('anomaly_value', {})
            if anomaly_values:
                most_anomalous = max(anomaly_values.items(), 
                                   key=lambda x: abs(x[1].get('z_score', 0)))
                column, stats = most_anomalous
                z_score = stats.get('z_score', 0)
                summary_parts.append(f"{column} reading of {stats['value']:.2f} is {abs(z_score):.1f}Ïƒ from normal")
            
            # Temporal context
            temporal_context = analysis.get('temporal_context', {})
            if 'time_period' in temporal_context:
                summary_parts.append(f"during {temporal_context['time_period']} hours")
            
            # Top recommendation
            recommendations = analysis.get('recommendations', [])
            if recommendations:
                summary_parts.append(f"Recommendation: {recommendations[0]}")
            
            return ". ".join(summary_parts) if summary_parts else "Anomaly detected - investigate sensor status"
            
        except Exception as e:
            logging.error(f"Summary generation failed: {e}")
            return "Anomaly detected - root cause analysis unavailable"


class StreamingDataProcessor:
    """
    Handles PySpark streaming data processing with privacy integration.
    """
    
    def __init__(self, spark_session: SparkSession, privacy_manager: 'PrivacyManager'):
        """
        Initialize streaming processor.
        
        Args:
            spark_session: Spark session for processing
            privacy_manager: Privacy manager for data protection
        """
        self.spark = spark_session
        self.privacy_manager = privacy_manager
        self.streaming_context = None
        self.batch_duration = 10  # seconds
        
    def setup_streaming_context(self, batch_duration: int = 10) -> None:
        """
        Setup Spark streaming context.
        
        Args:
            batch_duration: Batch duration in seconds
        """
        try:
            self.batch_duration = batch_duration
            self.streaming_context = StreamingContext(self.spark.sparkContext, batch_duration)
            logging.info(f"Streaming context initialized with {batch_duration}s batches")
            
        except Exception as e:
            logging.error(f"Streaming context setup failed: {e}")
            raise
    
    def process_streaming_data(self, input_stream_config: Dict[str, Any], 
                             anomaly_detector: AnomalyDetector,
                             alert_channels: List['AlertChannel']) -> None:
        """
        Process streaming data with privacy protection and anomaly detection.
        
        Args:
            input_stream_config: Configuration for input stream
            anomaly_detector: Trained anomaly detector
            alert_channels: List of alert channels for notifications
        """
        try:
            if not self.streaming_context:
                self.setup_streaming_context()
            
            # Define schema for streaming data
            schema = StructType([
                StructField("timestamp", TimestampType(), True),
                StructField("sensor_id", StringType(), True),
                StructField("value", DoubleType(), True),
                StructField("location", StringType(), True),
                StructField("customer_id", StringType(), True)
            ])
            
            # Create streaming DataFrame
            streaming_df = self.spark \
                .readStream \
                .format(input_stream_config.get('format', 'kafka')) \
                .option("kafka.bootstrap.servers", input_stream_config.get('kafka_servers', 'localhost:9092')) \
                .option("subscribe", input_stream_config.get('topic', 'sensor-data')) \
                .load()
            
            # Process each micro-batch
            def process_batch(batch_df: SparkDataFrame, batch_id: int):
                """Process each micro-batch with privacy protection."""
                try:
                    if batch_df.count() > 0:
                        logging.info(f"Processing batch {batch_id} with {batch_df.count()} records")
                        
                        # Convert to Pandas for processing
                        pandas_df = batch_df.toPandas()
                        
                        # Apply privacy protection
                        anonymized_df = self.privacy_manager.anonymize_sensor_data(
                            pandas_df, 
                            ['customer_id', 'location']
                        )
                        
                        # Group by sensor for anomaly detection
                        sensor_groups = anonymized_df.groupby('sensor_id')
                        
                        for sensor_id, sensor_data in sensor_groups:
                            try:
                                # Detect anomalies
                                anomalies = anomaly_detector.detect(sensor_data)
                                
                                # Process detected anomalies
                                for idx, confidence, anomaly_type in anomalies:
                                    alert = self._create_streaming_alert(
                                        sensor_id, 
                                        sensor_data.iloc[idx],
                                        confidence,
                                        anomaly_type,
                                        sensor_data
                                    )
                                    
                                    if alert:
                                        # Send alerts
                                        self._send_alerts_async(alert, alert_channels)
                                
                            except Exception as e:
                                logging.error(f"Anomaly detection failed for sensor {sensor_id}: {e}")
                        
                        # Log batch processing completion
                        self.privacy_manager._audit_logger.log_access(
                            user_id='streaming_system',
                            action='process_streaming_batch',
                            resource_type='sensor_data',
                            resource_id=f'batch_{batch_id}',
                            success=True,
                            details={'record_count': len(pandas_df), 'sensors': list(pandas_df['sensor_id'].unique())}
                        )
                        
                except Exception as e:
                    logging.error(f"Batch processing failed for batch {batch_id}: {e}")
                    
                    # Log failed batch
                    if hasattr(self, 'privacy_manager'):
                        self.privacy_manager._audit_logger.log_access(
                            user_id='streaming_system',
                            action='process_streaming_batch',
                            resource_type='sensor_data',
                            resource_id=f'batch_{batch_id}',
                            success=False,
                            details={'error': str(e)}
                        )
            
            # Start streaming with batch processing
            query = streaming_df.writeStream \
                .foreachBatch(process_batch) \
                .outputMode("append") \
                .option("checkpointLocation", input_stream_config.get('checkpoint_dir', '/tmp/streaming_checkpoint')) \
                .trigger(processingTime=f'{self.batch_duration} seconds') \
                .start()
            
            logging.info("Streaming query started")
            return query
            
        except Exception as e:
            logging.error(f"Streaming data processing setup failed: {e}")
            raise
    
    def _create_streaming_alert(self, sensor_id: str, anomaly_row: pd.Series,
                              confidence: float, anomaly_type: AnomalyType,
                              context_data: pd.DataFrame) -> Optional[AnomalyAlert]:
        """Create alert for streaming anomaly."""
        try:
            # Create root cause analyzer
            root_cause_analyzer = RootCauseAnalyzer()
            analysis = root_cause_analyzer.analyze(anomaly_row, context_data)
            
            # Determine severity
            if confidence > 0.8:
                severity = AlertSeverity.CRITICAL
            elif confidence > 0.6:
                severity = AlertSeverity.HIGH
            elif confidence > 0.4:
                severity = AlertSeverity.MEDIUM
            else:
                severity = AlertSeverity.LOW
            
            # Create alert
            alert = AnomalyAlert(
                alert_id=str(uuid.uuid4()),
                sensor_id=sensor_id,
                timestamp=datetime.now(),
                anomaly_type=anomaly_type,
                confidence=confidence,
                severity=severity,
                value=anomaly_row.get('value', 0.0),
                expected_range=(0.0, 100.0),  # Would be calculated from historical data
                root_cause_summary=root_cause_analyzer.generate_summary(analysis),
                feature_attributions=analysis.get('feature_contributions', {}),
                recommendations=analysis.get('recommendations', []),
                metadata={
                    'streaming_batch': True,
                    'processing_time': datetime.now().isoformat(),
                    'privacy_protected': True
                }
            )
            
            return alert
            
        except Exception as e:
            logging.error(f"Failed to create streaming alert: {e}")
            return None
    
    def _send_alerts_async(self, alert: AnomalyAlert, alert_channels: List['AlertChannel']) -> None:
        """Send alerts asynchronously."""
        try:
            for channel in alert_channels:
                try:
                    # Run alert sending in thread pool to avoid blocking
                    asyncio.create_task(channel.send_alert(alert))
                except Exception as e:
                    logging.error(f"Alert sending failed for channel {channel.__class__.__name__}: {e}")
                    
        except Exception as e:
            logging.error(f"Async alert sending setup failed: {e}")
    
    def stop_streaming(self) -> None:
        """Stop streaming context."""
        try:
            if self.streaming_context:
                self.streaming_context.stop(stopSparkContext=False)
                logging.info("Streaming context stopped")
                
        except Exception as e:
            logging.error(f"Failed to stop streaming context: {e}")


class AnomalyDetectionEngine:
    """
    Main anomaly detection engine with comprehensive functionality.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the anomaly detection engine.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        
        # Initialize components
        self.preprocessor = DataPreprocessor(config.get('preprocessing', {}))
        self.root_cause_analyzer = RootCauseAnalyzer(config.get('root_cause', {}))
        
        # Initialize detector based on config
        detector_type = config.get('detector_type', 'hybrid')
        detector_config = config.get('detector_config', {})
        
        if detector_type == 'statistical':
            self.detector = StatisticalAnomalyDetector(detector_config)
        elif detector_type == 'isolation_forest':
            self.detector = IsolationForestAnomalyDetector(detector_config)
        elif detector_type == 'seasonal':
            self.detector = SeasonalAnomalyDetector(detector_config)
        elif detector_type == 'adaptive_ewma':
            self.detector = AdaptiveEWMAStatisticalAnomalyDetector(detector_config)
        elif detector_type == 'incremental':
            self.detector = IncrementalIsolationForestDetector(detector_config)
        else:  # hybrid
            self.detector = HybridAnomalyDetector(detector_config)
        
        # Initialize Spark session
        self.spark = self._initialize_spark()
        
        # Alert channels will be set up separately
        self.alert_channels = []
        
        logging.info(f"Anomaly detection engine initialized with {detector_type} detector")
    
    def _initialize_spark(self) -> SparkSession:
        """Initialize Spark session for big data processing."""
        try:
            spark_config = self.config.get('spark', {})
            
            spark = SparkSession.builder \
                .appName("AnomalyDetectionEngine") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .getOrCreate()
            
            # Set log level
            spark.sparkContext.setLogLevel("WARN")
            
            logging.info("Spark session initialized")
            return spark
            
        except Exception as e:
            logging.error(f"Spark initialization failed: {e}")
            # Return None if Spark is not available
            return None
    
    def train(self, training_data: pd.DataFrame) -> None:
        """
        Train the anomaly detection model.
        
        Args:
            training_data: Historical data for training
        """
        try:
            # Preprocess training data
            processed_data = self.preprocessor.preprocess(training_data, 'training')
            
            # Train detector
            self.detector.fit(processed_data)
            
            logging.info(f"Model training completed on {len(processed_data)} samples")
            
        except Exception as e:
            logging.error(f"Model training failed: {e}")
            raise
    
    def detect_batch(self, data: pd.DataFrame, sensor_id: str) -> List[AnomalyAlert]:
        """
        Detect anomalies in batch data.
        
        Args:
            data: Input data for anomaly detection
            sensor_id: Sensor identifier
            
        Returns:
            List of anomaly alerts
        """
        try:
            # Preprocess data
            processed_data = self.preprocessor.preprocess(data, sensor_id)
            
            # Detect anomalies
            anomalies = self.detector.detect(processed_data)
            
            # Create alerts
            alerts = []
            for idx, confidence, anomaly_type in anomalies:
                alert = self._create_alert(
                    sensor_id, 
                    processed_data.iloc[idx],
                    confidence,
                    anomaly_type,
                    processed_data
                )
                if alert:
                    alerts.append(alert)
            
            logging.info(f"Batch detection completed: {len(alerts)} alerts generated")
            return alerts
            
        except Exception as e:
            logging.error(f"Batch anomaly detection failed: {e}")
            return []
    
    def _create_alert(self, sensor_id: str, anomaly_row: pd.Series,
                     confidence: float, anomaly_type: AnomalyType,
                     context_data: pd.DataFrame) -> Optional[AnomalyAlert]:
        """Create anomaly alert with comprehensive information."""
        try:
            # Perform root cause analysis
            feature_importances = self.detector.get_feature_importance()
            analysis = self.root_cause_analyzer.analyze(
                anomaly_row, context_data, feature_importances
            )
            
            # Determine severity based on confidence and anomaly type
            if confidence > 0.8 or anomaly_type == AnomalyType.CRITICAL:
                severity = AlertSeverity.CRITICAL
            elif confidence > 0.6:
                severity = AlertSeverity.HIGH
            elif confidence > 0.4:
                severity = AlertSeverity.MEDIUM
            else:
                severity = AlertSeverity.LOW
            
            # Calculate expected range (simplified)
            value = anomaly_row.get('value', 0.0)
            expected_range = self._calculate_expected_range(context_data, 'value')
            
            # Create alert
            alert = AnomalyAlert(
                alert_id=str(uuid.uuid4()),
                sensor_id=sensor_id,
                timestamp=anomaly_row.get('timestamp', datetime.now()),
                anomaly_type=anomaly_type,
                confidence=confidence,
                severity=severity,
                value=value,
                expected_range=expected_range,
                root_cause_summary=self.root_cause_analyzer.generate_summary(analysis),
                feature_attributions=analysis.get('feature_contributions', {}),
                recommendations=analysis.get('recommendations', []),
                metadata={
                    'detector': self.detector.name,
                    'processing_time': datetime.now().isoformat()
                }
            )
            
            return alert
            
        except Exception as e:
            logging.error(f"Alert creation failed: {e}")
            return None
    
    def _calculate_expected_range(self, data: pd.DataFrame, column: str) -> Tuple[float, float]:
        """Calculate expected range for a column."""
        try:
            if column in data.columns:
                series = data[column].dropna()
                if len(series) > 0:
                    q1 = series.quantile(0.25)
                    q3 = series.quantile(0.75)
                    iqr = q3 - q1
                    return (q1 - 1.5 * iqr, q3 + 1.5 * iqr)
            return (0.0, 100.0)  # Default range
            
        except Exception:
            return (0.0, 100.0)  # Default range
    
    def setup_streaming(self, stream_config: Dict[str, Any]) -> Optional[StreamingDataProcessor]:
        """
        Setup streaming data processing.
        
        Args:
            stream_config: Streaming configuration
            
        Returns:
            Streaming processor instance
        """
        try:
            if not self.spark:
                logging.error("Spark session not available for streaming")
                return None
            
            # This would be set up with privacy manager in the enhanced version
            privacy_manager = None  # Placeholder
            
            streaming_processor = StreamingDataProcessor(self.spark, privacy_manager)
            
            logging.info("Streaming processor setup completed")
            return streaming_processor
            
        except Exception as e:
            logging.error(f"Streaming setup failed: {e}")
            return None
    
    def add_alert_channel(self, channel: 'AlertChannel') -> None:
        """Add alert channel for notifications."""
        self.alert_channels.append(channel)
        logging.info(f"Added alert channel: {channel.__class__.__name__}")
    
    def shutdown(self) -> None:
        """Shutdown the engine and clean up resources."""
        try:
            if self.spark:
                self.spark.stop()
                logging.info("Spark session stopped")
            
            logging.info("Anomaly detection engine shutdown completed")
            
        except Exception as e:
            logging.error(f"Engine shutdown failed: {e}")


# Enhanced engine that integrates with privacy features (from previous response)
class EnhancedAnomalyDetectionEngine(AnomalyDetectionEngine):
    """
    Enhanced anomaly detection engine with comprehensive privacy features.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize enhanced engine with privacy features.
        
        Args:
            config: Configuration dictionary
        """
        # Initialize privacy manager first
        from __main__ import PrivacyManager  # Import from main module where it's defined
        self.privacy_manager = PrivacyManager()
        
        # Call parent initialization
        super().__init__(config)
        
        # Setup privacy-aware streaming
        if self.spark:
            self.streaming_processor = StreamingDataProcessor(self.spark, self.privacy_manager)
        
        # Setup privacy-aware alert channels
        self._setup_privacy_aware_alerts()
        
        logging.info("Enhanced anomaly detection engine with privacy features initialized")
    
    def _setup_privacy_aware_alerts(self) -> None:
        """Setup alert channels with credential encryption."""
        try:
            # Store credentials securely
            email_config = self.config.get('email', {})
            if email_config and 'password' in email_config:
                self.privacy_manager._credential_manager.store_credential(
                    'email_password', 
                    email_config['password']
                )
            
            sms_config = self.config.get('sms', {})
            if sms_config and 'api_key' in sms_config:
                self.privacy_manager._credential_manager.store_credential(
                    'sms_api_key',
                    sms_config['api_key']
                )
                
        except Exception as e:
            logging.error(f"Failed to setup privacy-aware alerts: {e}")
    
    def process_streaming_data(self, stream_config: Dict[str, Any]) -> None:
        """
        Process streaming data with privacy protection.
        
        Args:
            stream_config: Streaming configuration
        """
        try:
            if not self.streaming_processor:
                logging.error("Streaming processor not available")
                return
            
            # Start streaming with privacy protection
            query = self.streaming_processor.process_streaming_data(
                stream_config, 
                self.detector, 
                self.alert_channels
            )
            
            if query:
                logging.info("Privacy-protected streaming started")
                return query
            
        except Exception as e:
            logging.error(f"Privacy-aware streaming failed: {e}")
            raise


# Configuration helper
def create_engine_config() -> Dict[str, Any]:
    """Create default configuration for the anomaly detection engine."""
    return {
        'detector_type': 'hybrid',
        'detector_config': {
            'statistical': {
                'z_threshold': 3.0,
                'iqr_factor': 1.5
            },
            'isolation_forest': {
                'contamination': 0.1,
                'n_estimators': 100,
                'random_state': 42
            },
            'seasonal': {
                'seasonal_period': 24,
                'threshold_factor': 2.0
            },
            'adaptive_ewma': {
                'alpha': 0.3,
                'threshold_factor': 3.0,
                'adaptation_rate': 0.1
            },
            'ensemble_threshold': 0.5
        },
        'preprocessing': {
            'normalize': False,
            'handle_missing': True
        },
        'spark': {
            'app_name': 'AnomalyDetection',
            'master': 'local[*]'
        },
        'streaming': {
            'batch_duration': 10,
            'checkpoint_dir': '/tmp/streaming_checkpoint'
        }
    }


# Example usage
def demo_complete_engine():
    """Demonstrate the complete anomaly detection engine."""
    print("ðŸš€ COMPLETE ANOMALY DETECTION ENGINE DEMO")
    print("=" * 60)
    
    try:
        # Create configuration
        config = create_engine_config()
        
        # Initialize enhanced engine
        engine = EnhancedAnomalyDetectionEngine(config)
        
        # Create sample training data
        np.random.seed(42)
        training_data = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=1000, freq='H'),
            'value': np.random.normal(100, 10, 1000) + 10 * np.sin(np.arange(1000) * 2 * np.pi / 24),
            'sensor_id': ['sensor_001'] * 1000,
            'location': ['Building_A'] * 1000,
            'customer_id': ['CUST_001'] * 1000
        })
        
        # Add some anomalies to training data
        anomaly_indices = [100, 300, 500, 700, 900]
        for idx in anomaly_indices:
            training_data.loc[idx, 'value'] = training_data.loc[idx, 'value'] + np.random.choice([-50, 50])
        
        print(f"ðŸ“Š Training engine on {len(training_data)} samples...")
        engine.train(training_data)
        print("âœ… Training completed")
        
        # Create test data with anomalies
        test_data = pd.DataFrame({
            'timestamp': pd.date_range('2023-02-01', periods=100, freq='H'),
            'value': np.random.normal(100, 10, 100) + 10 * np.sin(np.arange(100) * 2 * np.pi / 24),
            'sensor_id': ['sensor_001'] * 100,
            'location': ['Building_A'] * 100,
            'customer_id': ['CUST_001'] * 100
        })
        
        # Inject test anomalies
        test_data.loc[20, 'value'] = 200  # High anomaly
        test_data.loc[50, 'value'] = 10   # Low anomaly
        
        print(f"\nðŸ” Processing test data with {len(test_data)} samples...")
        alerts = engine.detect_batch(test_data, 'sensor_001')
        
        # Display results
        print(f"âœ… Detection completed: {len(alerts)} alerts generated")
        
        for i, alert in enumerate(alerts[:3]):  # Show first 3 alerts
            print(f"\nðŸ“¢ Alert {i+1}:")
            print(f"  Type: {alert.anomaly_type.value}")
            print(f"  Confidence: {alert.confidence:.3f}")
            print(f"  Severity: {alert.severity.name}")
            print(f"  Value: {alert.value:.2f}")
            print(f"  Summary: {alert.root_cause_summary}")
            if alert.recommendations:
                print(f"  Recommendation: {alert.recommendations[0]}")
        
        # Test privacy features
        print(f"\nðŸ”’ Testing privacy features...")
        privacy_report = engine.generate_privacy_report()
        print(f"âœ… Privacy compliance level: {privacy_report.get('compliance_level', 'unknown')}")
        
        # Cleanup
        engine.shutdown()
        print("\nâœ… Complete engine demo finished successfully!")
        
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        logging.error(f"Complete engine demo failed: {e}")


if __name__ == "__main__":
    # Run complete engine demo
    demo_complete_engine()
